# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and Scikit-learn Logistic Regression model.
This model is then compared to an Azure AutoML run.

## Summary
This dataset contains data about UCI bank marketing, we will be doing classfication to predict if the client will subscribe to a term deposit with the bank. First, we will create and optimize an Scikit Logistic Regression ML pipeline, the hyperparamaters of which we will optimize using Hyperdrive. Then we will be comparing with the Auto ML results. 

In our experiment, the best performing model is stackensembleclassifier generated by AutoML.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
1. As part of our pipeline, we'll need a training script 'train.py'. Data is imported from the specified URL using 'TabularDatasetFactory'.
2. After doing the necessary data clean up, we split the data into train and test sets. 
3. First we need to get our 'workspace'  and 'experiment' objects running. Then we created a compute cluster with compute configuration 'vm_size' :'Standard_D2_V2' and   'max_nodes=4'.
4. Conda_dependencies.yml file is used for creating Sckit-learn environment.
5. Parameter Sampler - RandomSampling is used.
6. Early stopping policy is specified for saving training time and resources.
7. As Sklearn estimator is deprecated, ScriptRunConfig is used to define the configuration information needed to submit a run in Azure ML, including the script -'train.py', compute target - 'compute_cluster', environment - 'sklearn_env' , and any distributed job-specific configs.
8. With the help of HyperDriveConfig Class we define the configuration for hyperparameter space sampling, termination policy, primary metric, estimator, and the compute target to execute the experiment runs on. 

**What are the benefits of the parameter sampler you chose?**
In our experiment, Random Sampling is used as it supports early termination of low perfromance runs. In random sampling, hyperparameter values are randomly selected from the defined search space which may save cost and time.

**What are the benefits of the early stopping policy you chose?**
Early termination improves computational efficiency and when used with smaller allowable slack gives aggressive savings.Bandit terminates runs where the primary metrics is not within the specified slack amount compared to the best performing run.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
